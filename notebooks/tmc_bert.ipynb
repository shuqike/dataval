{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_warning()\n",
    "from transformers import AutoTokenizer\n",
    "from src.frameworks import TruncatedMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (C:/Users/AronQ/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.25it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\AronQ\\.cache\\huggingface\\datasets\\yelp_review_full\\yelp_review_full\\1.0.0\\e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf\\cache-aad1af4c7095bfa1.arrow\n",
      "Loading cached processed dataset at C:\\Users\\AronQ\\.cache\\huggingface\\datasets\\yelp_review_full\\yelp_review_full\\1.0.0\\e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf\\cache-29f27748f0b54d01.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\AronQ\\.cache\\huggingface\\datasets\\yelp_review_full\\yelp_review_full\\1.0.0\\e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf\\cache-11a7619c6a3c070f.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\AronQ\\.cache\\huggingface\\datasets\\yelp_review_full\\yelp_review_full\\1.0.0\\e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf\\cache-3c5c2a245be1b332.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(small_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(small_train_dataset['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell output should contain a warning like: \"Some weights of the model checkpoint at bert-base-cased were not used when initializing...\".\n",
    "\n",
    "This is OK as we are not using the pooler layer in order to compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dshap = TruncatedMC(train_dataset=small_train_dataset, X_train=small_train_dataset['text'], X_test=small_eval_dataset['text'], test_dataset=small_eval_dataset, model_family='bert-base-cased', num_labels=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the initial accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dshap.random_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wed May  3 01:03:05 2023'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate leave-one-out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.5826, 'train_samples_per_second': 0.603, 'train_steps_per_second': 0.121, 'train_loss': 1.646740436553955, 'epoch': 2.0}\n",
      "{'eval_loss': 1.9394981861114502, 'eval_accuracy': 0.4, 'eval_runtime': 3.279, 'eval_samples_per_second': 1.525, 'eval_steps_per_second': 0.305, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.0151, 'train_samples_per_second': 0.615, 'train_steps_per_second': 0.154, 'train_loss': 1.337770700454712, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:17<01:10, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4859471321105957, 'eval_accuracy': 0.2, 'eval_runtime': 3.23, 'eval_samples_per_second': 1.548, 'eval_steps_per_second': 0.31, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.4094, 'train_samples_per_second': 0.597, 'train_steps_per_second': 0.149, 'train_loss': 1.359778642654419, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:35<00:53, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7828117609024048, 'eval_accuracy': 0.4, 'eval_runtime': 3.314, 'eval_samples_per_second': 1.509, 'eval_steps_per_second': 0.302, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 14.311, 'train_samples_per_second': 0.559, 'train_steps_per_second': 0.14, 'train_loss': 1.4316271543502808, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:55<00:37, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2324886322021484, 'eval_accuracy': 0.2, 'eval_runtime': 3.458, 'eval_samples_per_second': 1.446, 'eval_steps_per_second': 0.289, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.3357, 'train_samples_per_second': 0.6, 'train_steps_per_second': 0.15, 'train_loss': 1.2421010732650757, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:13<00:18, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0160582065582275, 'eval_accuracy': 0.6, 'eval_runtime': 3.669, 'eval_samples_per_second': 1.363, 'eval_steps_per_second': 0.273, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 13.965, 'train_samples_per_second': 0.573, 'train_steps_per_second': 0.143, 'train_loss': 1.4319584369659424, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:33<00:00, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1640307903289795, 'eval_accuracy': 0.2, 'eval_runtime': 3.639, 'eval_samples_per_second': 1.374, 'eval_steps_per_second': 0.275, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\AronQ\\.conda\\envs\\flenv\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.598, 'train_samples_per_second': 0.602, 'train_steps_per_second': 0.12, 'train_loss': 1.4257745742797852, 'epoch': 2.0}\n",
      "{'eval_loss': 1.8735370635986328, 'eval_accuracy': 0.4, 'eval_runtime': 3.351, 'eval_samples_per_second': 1.492, 'eval_steps_per_second': 0.298, 'epoch': 2.0}\n",
      "{'eval_loss': 1.7414395809173584, 'eval_accuracy': 0.4, 'eval_runtime': 3.529, 'eval_samples_per_second': 1.417, 'eval_steps_per_second': 0.283, 'epoch': 2.0}\n",
      "{'eval_loss': 2.403810977935791, 'eval_accuracy': 0.0, 'eval_runtime': 3.554, 'eval_samples_per_second': 1.407, 'eval_steps_per_second': 0.281, 'epoch': 2.0}\n",
      "{'eval_loss': 2.002265691757202, 'eval_accuracy': 0.2, 'eval_runtime': 3.368, 'eval_samples_per_second': 1.485, 'eval_steps_per_second': 0.297, 'epoch': 2.0}\n",
      "{'eval_loss': 1.79889714717865, 'eval_accuracy': 0.4, 'eval_runtime': 3.523, 'eval_samples_per_second': 1.419, 'eval_steps_per_second': 0.284, 'epoch': 2.0}\n",
      "{'eval_loss': 2.403810977935791, 'eval_accuracy': 0.0, 'eval_runtime': 3.508, 'eval_samples_per_second': 1.425, 'eval_steps_per_second': 0.285, 'epoch': 2.0}\n",
      "{'eval_loss': 1.7161900997161865, 'eval_accuracy': 0.4, 'eval_runtime': 3.453, 'eval_samples_per_second': 1.448, 'eval_steps_per_second': 0.29, 'epoch': 2.0}\n",
      "{'eval_loss': 2.1177351474761963, 'eval_accuracy': 0.2, 'eval_runtime': 3.575, 'eval_samples_per_second': 1.399, 'eval_steps_per_second': 0.28, 'epoch': 2.0}\n",
      "{'eval_loss': 2.142430543899536, 'eval_accuracy': 0.2, 'eval_runtime': 3.428, 'eval_samples_per_second': 1.459, 'eval_steps_per_second': 0.292, 'epoch': 2.0}\n",
      "{'eval_loss': 2.1309945583343506, 'eval_accuracy': 0.0, 'eval_runtime': 3.368, 'eval_samples_per_second': 1.485, 'eval_steps_per_second': 0.297, 'epoch': 2.0}\n",
      "{'eval_loss': 2.109668254852295, 'eval_accuracy': 0.2, 'eval_runtime': 3.479, 'eval_samples_per_second': 1.437, 'eval_steps_per_second': 0.287, 'epoch': 2.0}\n",
      "{'eval_loss': 1.8735370635986328, 'eval_accuracy': 0.4, 'eval_runtime': 3.511, 'eval_samples_per_second': 1.424, 'eval_steps_per_second': 0.285, 'epoch': 2.0}\n",
      "{'eval_loss': 1.7414394617080688, 'eval_accuracy': 0.4, 'eval_runtime': 3.57, 'eval_samples_per_second': 1.401, 'eval_steps_per_second': 0.28, 'epoch': 2.0}\n",
      "{'eval_loss': 2.40325665473938, 'eval_accuracy': 0.0, 'eval_runtime': 3.429, 'eval_samples_per_second': 1.458, 'eval_steps_per_second': 0.292, 'epoch': 2.0}\n",
      "{'eval_loss': 1.7414395809173584, 'eval_accuracy': 0.4, 'eval_runtime': 3.8481, 'eval_samples_per_second': 1.299, 'eval_steps_per_second': 0.26, 'epoch': 2.0}\n",
      "{'eval_loss': 2.142430305480957, 'eval_accuracy': 0.2, 'eval_runtime': 3.591, 'eval_samples_per_second': 1.392, 'eval_steps_per_second': 0.278, 'epoch': 2.0}\n",
      "{'eval_loss': 1.7414395809173584, 'eval_accuracy': 0.4, 'eval_runtime': 3.596, 'eval_samples_per_second': 1.39, 'eval_steps_per_second': 0.278, 'epoch': 2.0}\n",
      "{'eval_loss': 2.4537556171417236, 'eval_accuracy': 0.0, 'eval_runtime': 3.562, 'eval_samples_per_second': 1.404, 'eval_steps_per_second': 0.281, 'epoch': 2.0}\n",
      "{'eval_loss': 2.321103811264038, 'eval_accuracy': 0.0, 'eval_runtime': 3.417, 'eval_samples_per_second': 1.463, 'eval_steps_per_second': 0.293, 'epoch': 2.0}\n",
      "{'eval_loss': 1.3726569414138794, 'eval_accuracy': 0.6, 'eval_runtime': 3.613, 'eval_samples_per_second': 1.384, 'eval_steps_per_second': 0.277, 'epoch': 2.0}\n",
      "{'eval_loss': 1.9448082447052002, 'eval_accuracy': 0.2, 'eval_runtime': 3.536, 'eval_samples_per_second': 1.414, 'eval_steps_per_second': 0.283, 'epoch': 2.0}\n",
      "{'eval_loss': 2.167679786682129, 'eval_accuracy': 0.2, 'eval_runtime': 3.5201, 'eval_samples_per_second': 1.42, 'eval_steps_per_second': 0.284, 'epoch': 2.0}\n",
      "{'eval_loss': 2.263646125793457, 'eval_accuracy': 0.0, 'eval_runtime': 3.457, 'eval_samples_per_second': 1.446, 'eval_steps_per_second': 0.289, 'epoch': 2.0}\n",
      "{'eval_loss': 2.084972858428955, 'eval_accuracy': 0.2, 'eval_runtime': 3.4998, 'eval_samples_per_second': 1.429, 'eval_steps_per_second': 0.286, 'epoch': 2.0}\n",
      "{'eval_loss': 2.40325665473938, 'eval_accuracy': 0.0, 'eval_runtime': 3.275, 'eval_samples_per_second': 1.527, 'eval_steps_per_second': 0.305, 'epoch': 2.0}\n",
      "{'eval_loss': 2.388230800628662, 'eval_accuracy': 0.0, 'eval_runtime': 2.849, 'eval_samples_per_second': 1.755, 'eval_steps_per_second': 0.351, 'epoch': 2.0}\n",
      "{'eval_loss': 1.8241466283798218, 'eval_accuracy': 0.4, 'eval_runtime': 3.621, 'eval_samples_per_second': 1.381, 'eval_steps_per_second': 0.276, 'epoch': 2.0}\n",
      "{'eval_loss': 2.027515172958374, 'eval_accuracy': 0.2, 'eval_runtime': 3.621, 'eval_samples_per_second': 1.381, 'eval_steps_per_second': 0.276, 'epoch': 2.0}\n",
      "{'eval_loss': 2.059723377227783, 'eval_accuracy': 0.2, 'eval_runtime': 3.415, 'eval_samples_per_second': 1.464, 'eval_steps_per_second': 0.293, 'epoch': 2.0}\n",
      "{'eval_loss': 2.0522103309631348, 'eval_accuracy': 0.2, 'eval_runtime': 3.417, 'eval_samples_per_second': 1.463, 'eval_steps_per_second': 0.293, 'epoch': 2.0}\n",
      "{'eval_loss': 2.0022659301757812, 'eval_accuracy': 0.2, 'eval_runtime': 3.4901, 'eval_samples_per_second': 1.433, 'eval_steps_per_second': 0.287, 'epoch': 2.0}\n",
      "{'eval_loss': 2.1923749446868896, 'eval_accuracy': 0.2, 'eval_runtime': 3.439, 'eval_samples_per_second': 1.454, 'eval_steps_per_second': 0.291, 'epoch': 2.0}\n",
      "{'eval_loss': 1.5627660751342773, 'eval_accuracy': 0.6, 'eval_runtime': 3.54, 'eval_samples_per_second': 1.412, 'eval_steps_per_second': 0.282, 'epoch': 2.0}\n",
      "{'eval_loss': 2.2170703411102295, 'eval_accuracy': 0.2, 'eval_runtime': 3.378, 'eval_samples_per_second': 1.48, 'eval_steps_per_second': 0.296, 'epoch': 2.0}\n",
      "{'eval_loss': 2.0602774620056152, 'eval_accuracy': 0.2, 'eval_runtime': 3.554, 'eval_samples_per_second': 1.407, 'eval_steps_per_second': 0.281, 'epoch': 2.0}\n",
      "{'eval_loss': 1.4800591468811035, 'eval_accuracy': 0.6, 'eval_runtime': 3.506, 'eval_samples_per_second': 1.426, 'eval_steps_per_second': 0.285, 'epoch': 2.0}\n",
      "{'eval_loss': 1.708677053451538, 'eval_accuracy': 0.4, 'eval_runtime': 3.424, 'eval_samples_per_second': 1.46, 'eval_steps_per_second': 0.292, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "dshap.run(save_every=100, err=1, do_loo=True, do_tmc=True, do_gshap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.asctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshap.vals_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dshap.vals_tmcshap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
